# app/api/endpoints/image_detection.py
from fastapi import APIRouter, HTTPException, File, UploadFile, Form, Request, Depends
from fastapi.responses import JSONResponse
import cv2
import numpy as np
import time
import logging
from typing import Optional, Dict, Any
from datetime import datetime
import io

from app.schemas.detection import DetectionRequest, DetectionResponse
from app.utils.image_utils import decode_base64_to_image, get_image_info, validate_image
from app.core.model_manager import ModelManager

router = APIRouter()
logger = logging.getLogger(__name__)

async def get_model_manager(request: Request) -> ModelManager:
    """R√©cup√®re le gestionnaire de mod√®les depuis l'√©tat de l'application"""
    return request.app.state.model_manager

@router.post("/image", response_model=DetectionResponse)
async def detect_objects_in_image(
    request: Request,
    file: Optional[UploadFile] = File(None),
    image_base64: Optional[str] = Form(None),
    model_name: Optional[str] = Form("stable_epoch_30"),
    confidence_threshold: Optional[float] = Form(0.5),
    nms_threshold: Optional[float] = Form(0.5),
    max_detections: Optional[int] = Form(50),
    enable_tracking: bool = Form(True),
    enable_lost_detection: bool = Form(True),
    model_manager: ModelManager = Depends(get_model_manager)
):
    """
    üîç D√©tecte les objets dans une image
    
    **Entr√©es support√©es:**
    - Upload de fichier image (multipart/form-data)
    - Image encod√©e en base64 (form field)
    
    **Param√®tres:**
    - model_name: Nom du mod√®le √† utiliser
    - confidence_threshold: Seuil de confiance (0.0-1.0)
    - nms_threshold: Seuil pour Non-Maximum Suppression
    - max_detections: Nombre maximum d'objets d√©tect√©s
    - enable_tracking: Activer le suivi d'objets
    - enable_lost_detection: Activer la d√©tection d'objets perdus
    """
    start_time = time.time()
    
    try:
        # Validation des entr√©es
        if not file and not image_base64:
            raise HTTPException(
                status_code=400,
                detail="Aucune image fournie. Utilisez 'file' ou 'image_base64'"
            )
        
        # Chargement de l'image
        if file:
            # Upload de fichier
            if not file.content_type.startswith('image/'):
                raise HTTPException(
                    status_code=400,
                    detail=f"Type de fichier non support√©: {file.content_type}"
                )
            
            # Lecture du fichier
            image_data = await file.read()
            nparr = np.frombuffer(image_data, np.uint8)
            image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
            
            if image is None:
                raise HTTPException(
                    status_code=400,
                    detail="Impossible de d√©coder l'image upload√©e"
                )
        
        else:
            # Image base64
            try:
                image = decode_base64_to_image(image_base64)
            except Exception as e:
                raise HTTPException(
                    status_code=400,
                    detail=f"Erreur d√©codage base64: {str(e)}"
                )
        
        # Validation de l'image
        if not validate_image(image):
            raise HTTPException(
                status_code=400,
                detail="Image invalide ou corrompue"
            )
        
        # Informations sur l'image
        img_info = get_image_info(image)
        logger.info(f"Image re√ßue: {img_info['width']}x{img_info['height']}, "
                   f"{img_info['size_bytes']/1024:.1f} KB")
        
        # Validation des param√®tres
        if not (0.0 <= confidence_threshold <= 1.0):
            raise HTTPException(
                status_code=400,
                detail="confidence_threshold doit √™tre entre 0.0 et 1.0"
            )
        
        if not (0.0 <= nms_threshold <= 1.0):
            raise HTTPException(
                status_code=400,
                detail="nms_threshold doit √™tre entre 0.0 et 1.0"
            )
        
        if not (1 <= max_detections <= 100):
            raise HTTPException(
                status_code=400,
                detail="max_detections doit √™tre entre 1 et 100"
            )
        
        # R√©cup√©ration du d√©tecteur
        try:
            detector = await model_manager.get_detector(model_name)
        except ValueError as e:
            raise HTTPException(
                status_code=400,
                detail=f"Mod√®le non disponible: {str(e)}"
            )
        except Exception as e:
            logger.error(f"Erreur chargement mod√®le {model_name}: {e}")
            raise HTTPException(
                status_code=500,
                detail=f"Erreur interne: impossible de charger le mod√®le"
            )
        
        # D√©tection
        detection_start = time.time()
        
        try:
            objects, persons, alerts = detector.detect(
                image,
                confidence_threshold=confidence_threshold,
                nms_threshold=nms_threshold,
                max_detections=max_detections,
                enable_tracking=enable_tracking,
                enable_lost_detection=enable_lost_detection
            )
        except Exception as e:
            logger.error(f"Erreur lors de la d√©tection: {e}")
            raise HTTPException(
                status_code=500,
                detail=f"Erreur lors de la d√©tection: {str(e)}"
            )
        
        detection_time = (time.time() - detection_start) * 1000  # en ms
        total_time = (time.time() - start_time) * 1000  # en ms
        
        # Mise √† jour des statistiques
        model_manager.update_inference_stats(model_name, detection_time)
        
        # Comptage des objets par statut
        lost_count = sum(1 for obj in objects if obj.status.value in ['lost', 'critical'])
        suspect_count = sum(1 for obj in objects if obj.status.value in ['suspect', 'surveillance'])
        
        # Cr√©ation de la r√©ponse
        response = DetectionResponse(
            success=True,
            timestamp=datetime.now(),
            processing_time=total_time,
            objects=objects,
            persons=persons,
            total_objects=len(objects),
            lost_objects=lost_count,
            suspect_objects=suspect_count,
            image_info=img_info,
            model_used=model_name
        )
        
        # Log des r√©sultats
        logger.info(f"D√©tection termin√©e: {len(objects)} objets, {len(persons)} personnes, "
                   f"{len(alerts)} alertes en {total_time:.1f}ms")
        
        if alerts:
            logger.warning(f"üö® {len(alerts)} alertes g√©n√©r√©es!")
            for alert in alerts:
                logger.warning(f"  - {alert.message}")
        
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erreur inattendue dans detect_objects_in_image: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Erreur interne du serveur: {str(e)}"
        )

@router.post("/image/analyze", response_model=Dict[str, Any])
async def analyze_image_detailed(
    request: Request,
    file: UploadFile = File(...),
    model_name: Optional[str] = Form("stable_epoch_30"),
    model_manager: ModelManager = Depends(get_model_manager)
):
    """
    üî¨ Analyse d√©taill√©e d'une image avec m√©trics avanc√©s
    
    Retourne des informations d√©taill√©es sur:
    - Qualit√© de l'image
    - Performance de d√©tection
    - Analyse spatiale des objets
    - Recommandations d'optimisation
    """
    start_time = time.time()
    
    try:
        # Validation du fichier
        if not file.content_type.startswith('image/'):
            raise HTTPException(
                status_code=400,
                detail=f"Type de fichier non support√©: {file.content_type}"
            )
        
        # Lecture de l'image
        image_data = await file.read()
        nparr = np.frombuffer(image_data, np.uint8)
        image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        
        if image is None:
            raise HTTPException(
                status_code=400,
                detail="Impossible de d√©coder l'image"
            )
        
        # Analyse de qualit√© d'image
        quality_metrics = _analyze_image_quality(image)
        
        # D√©tection avec diff√©rents seuils
        detector = await model_manager.get_detector(model_name)
        
        # Test avec diff√©rents seuils de confiance
        confidence_tests = [0.3, 0.5, 0.7, 0.9]
        detection_results = {}
        
        for conf_threshold in confidence_tests:
            objects, persons, alerts = detector.detect(
                image,
                confidence_threshold=conf_threshold,
                enable_tracking=False,
                enable_lost_detection=False
            )
            
            detection_results[f"confidence_{conf_threshold}"] = {
                "objects_count": len(objects),
                "persons_count": len(persons),
                "alerts_count": len(alerts),
                "avg_confidence": np.mean([obj.confidence for obj in objects]) if objects else 0
            }
        
        # Analyse spatiale
        spatial_analysis = _analyze_spatial_distribution(image, objects if 'objects' in locals() else [])
        
        # Recommandations
        recommendations = _generate_recommendations(quality_metrics, detection_results, spatial_analysis)
        
        total_time = (time.time() - start_time) * 1000
        
        return {
            "success": True,
            "analysis_time_ms": total_time,
            "image_quality": quality_metrics,
            "detection_performance": detection_results,
            "spatial_analysis": spatial_analysis,
            "recommendations": recommendations,
            "model_used": model_name
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erreur dans analyze_image_detailed: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Erreur lors de l'analyse: {str(e)}"
        )

def _analyze_image_quality(image: np.ndarray) -> Dict[str, Any]:
    """Analyse la qualit√© d'une image"""
    h, w = image.shape[:2]
    
    # Calcul de la nettet√© (variance du Laplacien)
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    sharpness = cv2.Laplacian(gray, cv2.CV_64F).var()
    
    # Calcul du contraste
    contrast = gray.std()
    
    # Luminosit√© moyenne
    brightness = gray.mean()
    
    # Distribution des couleurs
    hist = cv2.calcHist([image], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])
    color_diversity = (hist > 0).sum() / hist.size
    
    # √âvaluation globale
    quality_score = 0
    quality_factors = []
    
    if sharpness > 100:
        quality_score += 25
        quality_factors.append("‚úÖ Nettet√© correcte")
    else:
        quality_factors.append("‚ö†Ô∏è Image floue d√©tect√©e")
    
    if 50 < brightness < 200:
        quality_score += 25
        quality_factors.append("‚úÖ Luminosit√© correcte")
    else:
        quality_factors.append("‚ö†Ô∏è Probl√®me de luminosit√©")
    
    if contrast > 30:
        quality_score += 25
        quality_factors.append("‚úÖ Contraste suffisant")
    else:
        quality_factors.append("‚ö†Ô∏è Contraste faible")
    
    if color_diversity > 0.1:
        quality_score += 25
        quality_factors.append("‚úÖ Diversit√© des couleurs")
    else:
        quality_factors.append("‚ö†Ô∏è Image monotone")
    
    return {
        "resolution": {"width": w, "height": h, "megapixels": (w * h) / 1e6},
        "sharpness": float(sharpness),
        "contrast": float(contrast),
        "brightness": float(brightness),
        "color_diversity": float(color_diversity),
        "quality_score": quality_score,
        "quality_factors": quality_factors,
        "quality_level": "Excellente" if quality_score >= 75 else 
                        "Bonne" if quality_score >= 50 else
                        "Moyenne" if quality_score >= 25 else "Faible"
    }

def _analyze_spatial_distribution(image: np.ndarray, objects: list) -> Dict[str, Any]:
    """Analyse la distribution spatiale des objets"""
    h, w = image.shape[:2]
    
    if not objects:
        return {
            "objects_count": 0,
            "coverage": 0,
            "density": 0,
            "zones": {"top_left": 0, "top_right": 0, "bottom_left": 0, "bottom_right": 0}
        }
    
    # Zones de l'image
    zones = {"top_left": 0, "top_right": 0, "bottom_left": 0, "bottom_right": 0}
    total_area = 0
    
    for obj in objects:
        bbox = obj.bounding_box
        center_x = bbox.x + bbox.width / 2
        center_y = bbox.y + bbox.height / 2
        
        # Classification par zone
        if center_x < w/2 and center_y < h/2:
            zones["top_left"] += 1
        elif center_x >= w/2 and center_y < h/2:
            zones["top_right"] += 1
        elif center_x < w/2 and center_y >= h/2:
            zones["bottom_left"] += 1
        else:
            zones["bottom_right"] += 1
        
        # Aire totale couverte
        total_area += bbox.area()
    
    coverage = (total_area / (w * h)) * 100  # Pourcentage de couverture
    density = len(objects) / ((w * h) / 1000)  # Objets per 1000 pixels
    
    return {
        "objects_count": len(objects),
        "coverage_percent": float(coverage),
        "density": float(density),
        "zones": zones,
        "average_object_size": float(total_area / len(objects)) if objects else 0
    }

def _generate_recommendations(quality_metrics: dict, detection_results: dict, 
                            spatial_analysis: dict) -> List[str]:
    """G√©n√®re des recommandations d'optimisation"""
    recommendations = []
    
    # Recommandations qualit√© image
    if quality_metrics["quality_score"] < 50:
        recommendations.append("üîß Am√©liorer la qualit√© d'image pour de meilleures d√©tections")
        
        if quality_metrics["sharpness"] < 100:
            recommendations.append("üì∑ R√©duire le flou (stabilisation, mise au point)")
        
        if quality_metrics["brightness"] < 50:
            recommendations.append("üí° Augmenter l'√©clairage de la sc√®ne")
        elif quality_metrics["brightness"] > 200:
            recommendations.append("‚òÄÔ∏è R√©duire la surexposition")
        
        if quality_metrics["contrast"] < 30:
            recommendations.append("üîÜ Am√©liorer le contraste de l'image")
    
    # Recommandations seuils d√©tection
    conf_30 = detection_results.get("confidence_0.3", {})
    conf_70 = detection_results.get("confidence_0.7", {})
    
    if conf_30.get("objects_count", 0) > conf_70.get("objects_count", 0) * 2:
        recommendations.append("‚öñÔ∏è Consid√©rer un seuil de confiance plus √©lev√© (0.6-0.7)")
    
    if conf_70.get("objects_count", 0) == 0 and conf_30.get("objects_count", 0) > 0:
        recommendations.append("üìâ Consid√©rer un seuil de confiance plus bas (0.4-0.5)")
    
    # Recommandations spatiales
    if spatial_analysis["coverage_percent"] > 50:
        recommendations.append("üì¶ Sc√®ne dense - consid√©rer un NMS plus agressif")
    
    if spatial_analysis["density"] < 0.1:
        recommendations.append("üîç Sc√®ne sparse - v√©rifier si tous les objets sont d√©tect√©s")
    
    # Zone analysis
    zones = spatial_analysis.get("zones", {})
    max_zone = max(zones.values()) if zones else 0
    if max_zone > len(zones) * 0.7:  # Plus de 70% dans une zone
        recommendations.append("‚öñÔ∏è Objets concentr√©s dans une zone - v√©rifier l'angle de cam√©ra")
    
    return recommendations if recommendations else ["‚úÖ Configuration optimale d√©tect√©e"]